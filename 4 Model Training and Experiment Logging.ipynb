{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and experiment logging in Azure ML\n",
    "\n",
    "In this lab you are going to train a first ML model for the german credit dataset you have just registered. Please note the following characteristics of what we are about to do:\n",
    "\n",
    "- The ML model itself is trained using sklearn, python's most frequently used library for machine learning.\n",
    "- The computation of the model will happen inside of the Compute Instance.\n",
    "- The model training will be logged in an experiment in Azure ML\n",
    "- The final model will be registered in Azure ML\n",
    "\n",
    "\n",
    "## Connect to AML workspace & access data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve dataset\n",
    "\n",
    "In the following cell we are now going to retrieve the dataset we have previously registered. Note the seamless manner with which this happens. In the role of the data scientist, we don't need to know whether this data is stored, nor what account details must be used to access it. This is all abstracted by the Azure ML datasets. \n",
    "\n",
    "Using the *version* parameter, we can specify which version of the datset we wish to fetch. The version history is available through the python SDK but also in the Azure ML studio portal.\n",
    "\n",
    "Once the dataset object is instantiated, we then apply the *to_pandas_dataframe* method to it to turn it into a pandas dataframe which you can then use like any other pandas dataframe you have previously worked with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.get_by_name(ws, name='german_credit_dataset', version = \"latest\")\n",
    "ds_df = dataset.to_pandas_dataframe()\n",
    "ds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "The following cell creates a scatter plot that compares credit amounts with age of the people requesting the credit.\n",
    "\n",
    "Feel free to experiment with other plots to familiarize yourself with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ds_df.plot(kind='scatter', x='Age', y='Credit amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df.hist(column = 'Credit amount', by = 'Risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "Next, we are going to train a ML model to predict the *Risk* variable in our dataset. In essence, this will try to learn a system that can predict for a given set of input parameters whether a credit request is likely to be paid back or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ds_df.drop(\"Sno\", axis=1, inplace=True)\n",
    "\n",
    "y_raw = ds_df['Risk']\n",
    "X_raw = ds_df.drop('Risk', axis=1)\n",
    "\n",
    "categorical_features = X_raw.select_dtypes(include=['object']).columns\n",
    "numeric_features = X_raw.select_dtypes(include=['int64', 'float']).columns\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=\"missing\")),\n",
    "    ('onehotencoder', OneHotEncoder(categories='auto', sparse=False))])\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "feature_engineering_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric', numeric_transformer, numeric_features),\n",
    "        ('categorical', categorical_transformer, categorical_features)\n",
    "    ], remainder=\"drop\")\n",
    "\n",
    "# Encode Labels\n",
    "le = LabelEncoder()\n",
    "encoded_y = le.fit_transform(y_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, we split our dataset into training and testing subsets and then create a sklearn pipeline that will combine the pre-processing steps from before with the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_raw, encoded_y, test_size=0.20, stratify=encoded_y, random_state=42)\n",
    "\n",
    "# Create sklearn pipeline\n",
    "lr_clf = Pipeline(steps=[('preprocessor', feature_engineering_pipeline),\n",
    "                         ('classifier', LogisticRegression(solver=\"lbfgs\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using Azure ML's experiments functionality to keep track of all the models we trained as well as some of the important metrics we want to track. Here, we choose to tracking the model's accuracy on both the training and testing datasets. We could of course add other stuff to be tracked for our experiment run, e.g. the name of the algorithm that was used to train the model.\n",
    "\n",
    "Once this code cell has completed, head over to the *Experiments* sections in your [Azure ML studio](https://ml.azure.com) environment and look for your experiment as well as the tracked metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'german_credit_data_local'\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "run = experiment.start_logging()\n",
    "\n",
    "# Train the model\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# Capture metrics\n",
    "train_acc = lr_clf.score(X_train, y_train)\n",
    "test_acc = lr_clf.score(X_test, y_test)\n",
    "print(\"Training accuracy: %.3f\" % train_acc)\n",
    "print(\"Test data accuracy: %.3f\" % test_acc)\n",
    "\n",
    "# Log to Azure ML\n",
    "run.log('Train accuracy', train_acc)\n",
    "run.log('Test accuracy', test_acc)\n",
    "    \n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will download the trained model in a serialized format (using pickle) and then upload it as an attachment to the experiment run in Azure ML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(value=lr_clf, filename='model.pkl')\n",
    "\n",
    "# Upload our model to our experiment\n",
    "run.upload_file(name = 'outputs/model.pkl', path_or_stream = './model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model\n",
    "\n",
    "Another central piece of functionality in Azure ML is its model repository. Here we can register, version and manage trained models across projects. \n",
    "\n",
    "The following code cell with register our model. Once the cell has completed running, again head over to the  [Azure ML studio](https://ml.azure.com) and navigate to the *Models* page to see the model listed there. Note that here again we can add meta data tags. \n",
    "\n",
    "When clicking the model in the list of all models, a detail page will open. These page serves as the starting point for traceability where you can see what Run ID was used to create the model. You can jump to that particular Run ID to then see the code that was used to train the model, which user trained the model, what dataset was used etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='german-credit-local-model',\n",
    "                           model_path='outputs/model.pkl',\n",
    "                           datasets=[['training-dataset',dataset]],\n",
    "                           tags={\"use\": \"demo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to see how new model versions are created, re-submit your experiment and execute all the code up until model registration again. In the portal, you will then see a 2nd model listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Model Explaination\n",
    "\n",
    "Model explainations are very useful to improve ML models but also to help identify potential issues with your model, e.g. with regards to ethics.\n",
    "\n",
    "The following code cell will create a model explanation and attached it to your experiment run. To access it, wait for the code below to finish running and then again open your experiment in the Azure ML Studio and jump to the *Explanations (Preview)* section of the page. Take some minutes to play around with the explanation dashboard shown there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain model\n",
    "from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient\n",
    "from azureml.core.run import Run\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "#from azureml.contrib.interpret.visualize import ExplanationDashboard\n",
    "\n",
    "client = ExplanationClient.from_run(run)\n",
    "\n",
    "explainer = TabularExplainer(lr_clf.steps[-1][1], \n",
    "                             initialization_examples=X_train, \n",
    "                             features=X_raw.columns, \n",
    "                             classes=[\"Good\", \"Bad\"], \n",
    "                             transformations=feature_engineering_pipeline)\n",
    "\n",
    "# explain overall model predictions (global explanation)\n",
    "global_explanation = explainer.explain_global(X_test)\n",
    "\n",
    "# Sorted SHAP values\n",
    "print('ranked global importance values: {}'.format(global_explanation.get_ranked_global_values()))\n",
    "# Corresponding feature names\n",
    "print('ranked global importance names: {}'.format(global_explanation.get_ranked_global_names()))\n",
    "# Feature ranks (based on original order of features)\n",
    "print('global importance rank: {}'.format(global_explanation.global_importance_rank))\n",
    "\n",
    "client = ExplanationClient.from_run(run)\n",
    "client.upload_model_explanation(global_explanation, comment='global explanation: all features')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
